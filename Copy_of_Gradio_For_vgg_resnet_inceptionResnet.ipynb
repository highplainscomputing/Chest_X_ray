{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuzaifaAliHPC/Chest_X_ray-/blob/main/Copy_of_Gradio_For_vgg_resnet_inceptionResnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function 1\n",
        "This function gets data from *HuggingFace*"
      ],
      "metadata": {
        "id": "LzpcgxVZdswe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_huggingface():\n",
        "  !pip install gradio\n",
        "  !curl -Lo /content/archive.zip https://huggingface.co/datasets/HuzaifaHPC/chest_X_ray/resolve/main/archive.zip\n",
        "  !curl -Lo /content/InceptionResnet_Chest_X_ray.h5 https://huggingface.co/HuzaifaHPC/INCRES_Chest_X_ray_3.h5/resolve/main/InceptionResnet_Chest_X_ray.h5\n",
        "  !curl -Lo /content/VGG16_Chest_X_ray.h5 https://huggingface.co/HuzaifaHPC/VGG16_Chest_X_Ray/resolve/main/VGG16_Chest_X_ray.h5\n",
        "  !curl -Lo /content/Resnet_Chest_X_ray.h5 https://huggingface.co/HuzaifaHPC/Resnet_Chest_X_Ray/resolve/main/Resnet_Chest_X_ray.h5\n",
        "  !git clone https://github.com/HuzaifaAliHPC/Chest_X_ray-\n",
        "  !unzip /content/archive.zip\n",
        "\n",
        "get_data_huggingface()"
      ],
      "metadata": {
        "id": "c-520MgJZAOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "V2ebH2OFd8Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.image as mpimg\n",
        "from platform import python_version\n",
        "from PIL import Image\n",
        "# from IPython.display import Image, display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.applications import vgg16, ResNet152V2, InceptionResNetV2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, ImageDataGenerator\n",
        "import gradio as gr\n",
        "warnings.simplefilter(action='ignore',category=FutureWarning)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "print(f'TensorFlow version: {tf.__version__}')\n",
        "print(f'Python version: {python_version()}')"
      ],
      "metadata": {
        "id": "9TAnH5ItqBQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4894d29-0cad-4df8-ea60-3daa98234c8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.13.0\n",
            "Python version: 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "ZYM5qJTteKcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create directories for training, testing and validation data"
      ],
      "metadata": {
        "id": "eCpj0TaSd_Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "BASE_PATH = \"/content/chest_xray/\"\n",
        "TRAIN_PATH = BASE_PATH + \"train/\"\n",
        "TEST_PATH = BASE_PATH + \"test/\"\n",
        "VAL_PATH = BASE_PATH + \"val/\"\n",
        "\n",
        "SHAPE = (224,224,3)\n",
        "batch_size = 64\n",
        "classes = [\"NORMAL\", \"PNEUMONIA\"]\n",
        "\n",
        "print(f'train_path: {TRAIN_PATH}')\n",
        "print(f'test_path: {TEST_PATH}')\n",
        "print(f'val_path: {VAL_PATH}')"
      ],
      "metadata": {
        "id": "rrOrjDRzqFNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86efc17b-0357-4815-95f0-c412f2cd5962"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_path: /content/chest_xray/train/\n",
            "test_path: /content/chest_xray/test/\n",
            "val_path: /content/chest_xray/val/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pneumonia_data_dir = TEST_PATH + classes[1]\n",
        "# global variables\n",
        "random_images = [os.path.join(pneumonia_data_dir, filename) for filename in random.sample([filename for filename in os.listdir(pneumonia_data_dir) if filename.endswith(\".jpeg\")], 4)]\n",
        "sample_images = [\n",
        "                  random_images\n",
        "\n",
        "                 ]\n",
        "classes = [\"NORMAL\", \"PNEUMONIA\"]"
      ],
      "metadata": {
        "id": "5DOwK_73-Mdk"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting lengths of each directories"
      ],
      "metadata": {
        "id": "auORNgkVeD80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = len(os.listdir(os.path.join(TRAIN_PATH, classes[0]))) + len(os.listdir(os.path.join(TRAIN_PATH, classes[1])))\n",
        "test_data = len(os.listdir(os.path.join(TEST_PATH, classes[0]))) + len(os.listdir(os.path.join(TEST_PATH, classes[1])))\n",
        "print(training_data, test_data)"
      ],
      "metadata": {
        "id": "665J-N1Hw06S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb91d473-1857-4e03-be9d-e9faad7fb43e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5216 624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1/255,\n",
        "                                  featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "                                  samplewise_center=False,  # set each sample mean to 0\n",
        "                                  featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "                                  samplewise_std_normalization=False,  # divide each input by its std\n",
        "                                  zca_whitening=False,  # apply ZCA whitening\n",
        "                                  rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "                                  zoom_range = 0.2, # Randomly zoom image\n",
        "                                  width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "                                  height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "                                  horizontal_flip = True,  # randomly flip images\n",
        "                                  vertical_flip=False)  # randomly flip images)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255)\n",
        "val_datagen = ImageDataGenerator(rescale=1/255)"
      ],
      "metadata": {
        "id": "RZBJo3Krcp5O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "                  TRAIN_PATH,\n",
        "                  target_size = (SHAPE[0],SHAPE[1]),\n",
        "                  batch_size = batch_size,\n",
        "                  class_mode = 'categorical',\n",
        "                  shuffle = True,\n",
        "                  subset = None,\n",
        "                  seed = 33\n",
        "                  )"
      ],
      "metadata": {
        "id": "14t4YFSqcq2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0319572d-8e60-4f4f-a812-d326eba6a988"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "                TEST_PATH,\n",
        "                target_size = (SHAPE[0],SHAPE[1]),\n",
        "                batch_size = batch_size,\n",
        "                class_mode = 'categorical',\n",
        "                shuffle = True,\n",
        "                subset = None,\n",
        "                seed = 33\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "                VAL_PATH,\n",
        "                target_size = (SHAPE[0],SHAPE[1]),\n",
        "                batch_size = batch_size,\n",
        "                class_mode = 'categorical',\n",
        "                shuffle = True,\n",
        "                subset = None,\n",
        "                seed = 33\n",
        ")"
      ],
      "metadata": {
        "id": "6yArChK2csG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed69951-5eb2-42d5-fb48-c5244be22365"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 624 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate test labels for predictions, confusion matrix"
      ],
      "metadata": {
        "id": "S-JKPdiweNut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_num = test_generator.samples\n",
        "\n",
        "label_test = []\n",
        "\n",
        "for i in range((test_num // test_generator.batch_size)+1):\n",
        "  X,y = test_generator.next()\n",
        "  label_test.append(y)\n",
        "label_test = np.argmax(np.vstack(label_test), axis=1)\n",
        "label_test.shape"
      ],
      "metadata": {
        "id": "8jhHRF1fcvje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd303de5-bf29-4de8-858c-59dc4194fa82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(624,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# for VGG16\n",
        "\n",
        "1. training\n",
        "2. GradCAM\n",
        "\n"
      ],
      "metadata": {
        "id": "IIOsP3ZFcwHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will run training scripts and save VGG16 model to /content/model.h5\n",
        "def vgg_train():\n",
        "  !python /content/Chest_X_ray-/vgg_training.py"
      ],
      "metadata": {
        "id": "cJ_cMxzQuSQB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vgg():\n",
        "  return tf.keras.models.load_model('/content/VGG16_Chest_X_ray.h5')"
      ],
      "metadata": {
        "id": "2aEHRPhvok5R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def VGG_test_accuracy():\n",
        "  vgg_model = load_vgg()\n",
        "  report_vgg = classification_report(label_test, np.argmax(vgg_model.predict(test_generator),axis=1))\n",
        "  VGG_test_accuracy = float(report_vgg.split()[-2])\n",
        "  return VGG_test_accuracy\n"
      ],
      "metadata": {
        "id": "2JW7JC88qnfV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vgg_CM():\n",
        "  vgg_model = load_vgg()\n",
        "  y_preds_VGG16 = np.argmax(vgg_model.predict(test_generator),axis=1)\n",
        "\n",
        "  cm_VGG16 = confusion_matrix(label_test, y_preds_VGG16)\n",
        "  cmp_VGG16 = ConfusionMatrixDisplay(cm_VGG16, display_labels=classes)\n",
        "  fig, ax = plt.subplots(figsize=(5,5))\n",
        "  cmp_VGG16.plot(ax=ax)\n",
        "  plt.xlabel('Predicted Class for VGG16')\n",
        "  plt.ylabel('True Class')\n",
        "  plt.savefig(\"pneumonia_confusion_matrix_for_VGG16.png\", bbox_inches='tight')\n",
        "\n",
        "heatmaps_pneumonia = []\n",
        "img_path = \"/content/chest_xray/test/PNEUMONIA/person100_bacteria_481.jpeg\""
      ],
      "metadata": {
        "id": "jPODZ1oDqoxF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_vgg(image):\n",
        "\n",
        "    model = load_vgg()\n",
        "    image = image.reshape((-1, 224, 224, 3))\n",
        "    image = tf.keras.applications.vgg16.preprocess_input(image)\n",
        "    prediction1 = model.predict(image).flatten()\n",
        "    return {classes[i]: float(prediction1[i]) for i in range(2)}"
      ],
      "metadata": {
        "id": "mLASIxWPo15b"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_img_array(img_path, size):\n",
        "\n",
        "    img = keras.utils.load_img(img_path, target_size=size)\n",
        "\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 224, 224, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap_vgg16(img_array, last_conv_layer_name,  model = load_vgg(), pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "\n",
        "    grad_model = keras.models.Model(\n",
        "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "img_size = SHAPE\n",
        "last_conv_layer_name_VGG16 = \"block5_conv3\"\n",
        "preprocess_input_vgg16 = keras.applications.vgg16.preprocess_input\n",
        "# Prepare image\n",
        "# img_array = preprocess_input_vgg16(get_img_array(img_path, size=img_size))\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def save_and_display_gradcam_vgg(img_path, cam_path=\"cam.jpg\", alpha=0.4):\n",
        "    # Load the original image\n",
        "    img = keras.utils.load_img(img_path)\n",
        "    img = keras.utils.img_to_array(img)\n",
        "    img_array = preprocess_input_vgg16(get_img_array(img_path, size=img_size))\n",
        "    heatmap= make_gradcam_heatmap_vgg16(img_array, last_conv_layer_name_VGG16)\n",
        "    # Rescale heatmap to a range 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    pred = pred_vgg(get_img_array(img_path, size=img_size))\n",
        "\n",
        "    # Use jet colormap to colorize heatmap\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "    # Use RGB values of the colormap\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "    # Create an image with RGB colorized heatmap\n",
        "    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
        "\n",
        "    # Superimpose the heatmap on original image\n",
        "    superimposed_img = jet_heatmap * alpha + img\n",
        "    superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
        "\n",
        "\n",
        "\n",
        "    # Save the superimposed image\n",
        "    superimposed_img.save(cam_path)\n",
        "\n",
        "    # Display Grad CAM\n",
        "    return Image.open(cam_path).resize((224,224)), pred\n",
        "\n",
        "save_and_display_gradcam_vgg(img_path)"
      ],
      "metadata": {
        "id": "ZzQSLCMqUXZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec54315-9381-47a1-95c3-091f4d9d05df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-ae5e3178c947>:68: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
            "  jet = cm.get_cmap(\"jet\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=224x224>,\n",
              " {'NORMAL': 0.9999352693557739, 'PNEUMONIA': 6.468516221502796e-05})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "def CM_VGG16():\n",
        "  vgg_CM()\n",
        "  return Image.open(\"/content/pneumonia_confusion_matrix_for_VGG16.png\")"
      ],
      "metadata": {
        "id": "QvF2r6N7Ny6r"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_info_VGG16():\n",
        "  # model_info = dict(training_data = training_data,\n",
        "  #                   test_data = test_data,\n",
        "  #                   test_accuracy = VGG_test_accuracy())\n",
        "  return training_data, classes, test_data, VGG_test_accuracy()"
      ],
      "metadata": {
        "id": "YthBbAn_ylty"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_info_VGG16():\n",
        "  model_name = \"VGG16\"\n",
        "  model_size =  os.path.getsize('/content/VGG16_Chest_X_ray.h5') / (1024 * 1024)\n",
        "  model_size = \"{:.2f}\".format(model_size) + \" \" + \"MB\"\n",
        "  image_source = \"imagenet\"\n",
        "  return model_name, model_size, image_source"
      ],
      "metadata": {
        "id": "lrKY4Io5eGgs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# for Resnet\n",
        "\n",
        "1. training\n",
        "2. GradCAM"
      ],
      "metadata": {
        "id": "qY4JqUF7wug1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_train():\n",
        "  !python /content/Chest_X_ray-/resnet_training.py"
      ],
      "metadata": {
        "id": "B-n2npK-wsEU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_res():\n",
        "  return tf.keras.models.load_model('/content/Resnet_Chest_X_ray.h5')"
      ],
      "metadata": {
        "id": "OO160xyPwsBF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RESNET_test_accuracy():\n",
        "  resnet_model = load_res()\n",
        "  report_resnet = classification_report(label_test, np.argmax(resnet_model.predict(test_generator),axis=1))\n",
        "  RESNET_test_accuracy = float(report_resnet.split()[-2])\n",
        "  return RESNET_test_accuracy\n",
        "# print(RESNET_test_accuracy)"
      ],
      "metadata": {
        "id": "ePZwSSXRwr-N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def res_CM():\n",
        "  resnet_model = load_res()\n",
        "  y_preds_RESNET = np.argmax(resnet_model.predict(test_generator),axis=1)\n",
        "\n",
        "  cm_RESNET = confusion_matrix(label_test, y_preds_RESNET)\n",
        "  cmp_RESNET = ConfusionMatrixDisplay(cm_RESNET, display_labels=classes)\n",
        "  fig, ax = plt.subplots(figsize=(5,5))\n",
        "  cmp_RESNET.plot(ax=ax)\n",
        "  plt.xlabel('Predicted Class for RESNET')\n",
        "  plt.ylabel('True Class')\n",
        "  plt.savefig(\"pneumonia_confusion_matrix_for_RESNET.png\", bbox_inches='tight')\n",
        "\n",
        "\n",
        "heatmaps_pneumonia = []\n",
        "img_path = \"/content/chest_xray/test/PNEUMONIA/person100_bacteria_481.jpeg\""
      ],
      "metadata": {
        "id": "MTxvoqqfwr7U"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_res(image):\n",
        "\n",
        "    model = load_res()\n",
        "    image = image.reshape((-1, 224, 224, 3))\n",
        "    image = tf.keras.applications.resnet_v2.preprocess_input(image)\n",
        "    prediction1 = model.predict(image).flatten()\n",
        "    return {classes[i]: float(prediction1[i]) for i in range(2)}"
      ],
      "metadata": {
        "id": "M066KHN-dCz0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_img_array(img_path, size):\n",
        "    # `img` is a PIL image of size 299x299\n",
        "    img = keras.utils.load_img(img_path, target_size=size)\n",
        "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 299, 299, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap_resnet(img_array, last_conv_layer_name, model = load_res() , pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    grad_model = keras.models.Model(\n",
        "        model.inputs, [model.get_layer(last_conv_layer_name_RESNET).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "img_size = SHAPE\n",
        "last_conv_layer_name_RESNET = \"conv5_block3_3_conv\"\n",
        "preprocess_input_RESNET = tf.keras.applications.resnet_v2.preprocess_input\n",
        "# Prepare image\n",
        "# img_array = preprocess_input_RESNET(get_img_array(img_path, size=img_size))\n",
        "\n",
        "\n",
        "\n",
        "def save_and_display_gradcam_resnset(img_path, cam_path=\"cam.jpg\", alpha=0.4):\n",
        "    # Load the original image\n",
        "    img = keras.utils.load_img(img_path)\n",
        "    img = keras.utils.img_to_array(img)\n",
        "    img_array = preprocess_input_RESNET(get_img_array(img_path, size=img_size))\n",
        "    heatmap = make_gradcam_heatmap_resnet(img_array, last_conv_layer_name_RESNET)\n",
        "    # Rescale heatmap to a range 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    pred = pred_res(get_img_array(img_path, size=img_size))\n",
        "\n",
        "    # Use jet colormap to colorize heatmap\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "    # Use RGB values of the colormap\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "    # Create an image with RGB colorized heatmap\n",
        "    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
        "\n",
        "    # Superimpose the heatmap on original image\n",
        "    superimposed_img = jet_heatmap * alpha + img\n",
        "    superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
        "\n",
        "    # Save the superimposed image\n",
        "    superimposed_img.save(cam_path)\n",
        "\n",
        "    # Display Grad CAM\n",
        "    return Image.open(cam_path).resize((224,224)), pred\n",
        "\n",
        "\n",
        "# save_and_display_gradcam_resnset(img_path)"
      ],
      "metadata": {
        "id": "RHFONqIUwr4U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "def CM_RESNET():\n",
        "  res_CM()\n",
        "  return Image.open(\"/content/pneumonia_confusion_matrix_for_RESNET.png\")"
      ],
      "metadata": {
        "id": "92txfmK3z1uS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_info_RESNET():\n",
        "  # model_info = dict(training_data = training_data,\n",
        "  #                   test_data = test_data,\n",
        "  #                   test_accuracy = RESNET_test_accuracy())\n",
        "  return training_data, classes, test_data, RESNET_test_accuracy()"
      ],
      "metadata": {
        "id": "Tb34hx9Gz5-z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_info_RESNET():\n",
        "  model_name = \"ResNet152V2\"\n",
        "  model_size =  os.path.getsize('/content/Resnet_Chest_X_ray.h5') / (1024 * 1024)\n",
        "  model_size = \"{:.2f}\".format(model_size) + \" \" + \"MB\"\n",
        "  image_source = \"imagenet\"\n",
        "  return model_name, model_size, image_source"
      ],
      "metadata": {
        "id": "ugvyL8dCfdML"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InceptionResnet\n",
        "\n",
        "1. training\n",
        "2. GradCAM"
      ],
      "metadata": {
        "id": "jJlHoFmhyWJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inceptionResnet_train():\n",
        "  !python /content/Chest_X_ray-/inception_resnet_training.py"
      ],
      "metadata": {
        "id": "pSBrReaxwr1P"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_incres():\n",
        "  return tf.keras.models.load_model('/content/InceptionResnet_Chest_X_ray.h5')"
      ],
      "metadata": {
        "id": "48i9VxDywryc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def INCRES_test_accuracy():\n",
        "  incres_model = load_incres()\n",
        "  report_incres = classification_report(label_test, np.argmax(incres_model.predict(test_generator),axis=1))\n",
        "  INCRES_test_accuracy = float(report_incres.split()[-2])\n",
        "  return INCRES_test_accuracy\n",
        "# print(INCRES_test_accuracy)"
      ],
      "metadata": {
        "id": "UdryZ0rXwrvd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def incres_CM():\n",
        "  incres_model = load_incres()\n",
        "  y_preds_INCRES = np.argmax(incres_model.predict(test_generator),axis=1)\n",
        "\n",
        "  cm_INCRES = confusion_matrix(label_test, y_preds_INCRES)\n",
        "  cmp_INCRES = ConfusionMatrixDisplay(cm_INCRES, display_labels=classes)\n",
        "  fig, ax = plt.subplots(figsize=(5,5))\n",
        "  cmp_INCRES.plot(ax=ax)\n",
        "  plt.xlabel('Predicted Class for INCRES')\n",
        "  plt.ylabel('True Class')\n",
        "  plt.savefig(\"pneumonia_confusion_matrix_for_INCRES.png\", bbox_inches='tight')\n",
        "\n",
        "heatmaps_pneumonia = []\n",
        "img_path = \"/content/chest_xray/test/PNEUMONIA/person100_bacteria_481.jpeg\""
      ],
      "metadata": {
        "id": "BNTXF22NwrS0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_res(image):\n",
        "\n",
        "    model = load_incres()\n",
        "    image = image.reshape((-1, 224, 224, 3))\n",
        "    image = tf.keras.applications.inception_resnet_v2.preprocess_input(image)\n",
        "    prediction1 = model.predict(image).flatten()\n",
        "    return {classes[i]: float(prediction1[i]) for i in range(2)}"
      ],
      "metadata": {
        "id": "l1XZqzoHdU2M"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_img_array(img_path, size):\n",
        "    # `img` is a PIL image of size 299x299\n",
        "    img = keras.utils.load_img(img_path, target_size=size)\n",
        "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 299, 299, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap_incres(img_array, last_conv_layer_name, model = load_incres(), pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    grad_model = keras.models.Model(\n",
        "        model.inputs, [model.get_layer(last_conv_layer_name_INCRES).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "img_size = SHAPE\n",
        "last_conv_layer_name_INCRES = \"conv_7b\"\n",
        "preprocess_input_INCRES = tf.keras.applications.inception_resnet_v2.preprocess_input\n",
        "# Prepare image\n",
        "img_array = preprocess_input_INCRES(get_img_array(img_path, size=img_size))\n",
        "\n",
        "\n",
        "def save_and_display_gradcam_incres(img_path, cam_path=\"cam.jpg\", alpha=0.4):\n",
        "    # Load the original image\n",
        "    img = keras.utils.load_img(img_path)\n",
        "    img = keras.utils.img_to_array(img)\n",
        "    img_array = preprocess_input_INCRES(get_img_array(img_path, size=img_size))\n",
        "    heatmap = make_gradcam_heatmap_incres(img_array, last_conv_layer_name_INCRES)\n",
        "    # Rescale heatmap to a range 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    pred = pred_res(get_img_array(img_path, size=img_size))\n",
        "\n",
        "    # Use jet colormap to colorize heatmap\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "    # Use RGB values of the colormap\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "    # Create an image with RGB colorized heatmap\n",
        "    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
        "\n",
        "    # Superimpose the heatmap on original image\n",
        "    superimposed_img = jet_heatmap * alpha + img\n",
        "    superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
        "\n",
        "    # Save the superimposed image\n",
        "    superimposed_img.save(cam_path)\n",
        "\n",
        "    # Display Grad CAM\n",
        "    return Image.open(cam_path).resize((224,224)), pred\n",
        "\n",
        "\n",
        "#save_and_display_gradcam_incres(img_path)"
      ],
      "metadata": {
        "id": "7eKiwky8ycl1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "def CM_INCRES():\n",
        "  incres_CM()\n",
        "  return Image.open(\"/content/pneumonia_confusion_matrix_for_INCRES.png\")"
      ],
      "metadata": {
        "id": "-auPpF29yci0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_info_INCRES():\n",
        "  # model_info = dict(training_data = training_data,\n",
        "  #                   test_data = test_data,\n",
        "  #                   test_accuracy = INCRES_test_accuracy())\n",
        "  return training_data, classes, test_data, INCRES_test_accuracy()"
      ],
      "metadata": {
        "id": "ecvOp0aGycgc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_info_INCRES():\n",
        "  model_name = \"InceptionResNetV2\"\n",
        "  model_size =  os.path.getsize('/content/InceptionResnet_Chest_X_ray.h5') / (1024 * 1024)\n",
        "  model_size = \"{:.2f}\".format(model_size) + \" \" + \"MB\"\n",
        "\n",
        "  image_source = \"imagenet\"\n",
        "  return model_name, model_size, image_source"
      ],
      "metadata": {
        "id": "ZMrG3tQ3fwHj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradio UI"
      ],
      "metadata": {
        "id": "CcEXVFMQyem8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gr.inputs.Image??"
      ],
      "metadata": {
        "id": "EEmGj8orCUWG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "  with gr.Tab(\"VGG16\"):\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              training_data_vgg = gr.components.Textbox(label = \"Training Data\")\n",
        "              classes_vgg = gr.components.Textbox(label = \"Number of Classes\")\n",
        "              test_data_vgg = gr.components.Textbox(label = \"Testing Data\")\n",
        "              test_accuracy_vgg = gr.components.Textbox(label = \"Test Accuracy\")\n",
        "      btn = gr.Button(\"Get Data Info\")\n",
        "      btn.click(data_info_VGG16, outputs=[training_data_vgg, classes_vgg, test_data_vgg, test_accuracy_vgg])\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              model_name_vgg = gr.components.Textbox(label = \"Model Name\")\n",
        "              model_size_vgg = gr.components.Textbox(label = \"Model Size\")\n",
        "              image_source_vgg = gr.components.Textbox(label = \"Image Source\")\n",
        "      btn = gr.Button(\"Get Model Info\")\n",
        "      btn.click(model_info_VGG16, outputs=[model_name_vgg, model_size_vgg, image_source_vgg])\n",
        "\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              out_vgg = gr.components.Textbox()\n",
        "      btn = gr.Button(\"VGG retrain\")\n",
        "      btn.click(vgg_train, outputs=[out_vgg])\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              out_cm_vgg = gr.components.Image(type='filepath')\n",
        "      btn = gr.Button(\"Confusion Matrix\")\n",
        "      btn.click(CM_VGG16, outputs=[out_cm_vgg])\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              seed_vgg = gr.components.Image(type='filepath')\n",
        "          with gr.Column():\n",
        "              out_vgg = gr.components.Image(type='filepath')\n",
        "              label = gr.components.Label(num_top_classes = 2)\n",
        "\n",
        "      btn = gr.Button(\"GradCAM\")\n",
        "      btn.click(save_and_display_gradcam_vgg, inputs=[seed_vgg], outputs=[out_vgg, label])\n",
        "      gr.Examples(sample_images, inputs=[seed_vgg])\n",
        "\n",
        "# for Resnet\n",
        "  with gr.Tab(\"ResNet152V2\"):\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              training_data_res = gr.components.Textbox(label = \"Training Data\")\n",
        "              classes_res = gr.components.Textbox(label = \"Number of Classes\")\n",
        "              test_data_res = gr.components.Textbox(label = \"Testing Data\")\n",
        "              test_accuracy_res = gr.components.Textbox(label = \"Test Accuracy\")\n",
        "      btn = gr.Button(\"Get Data Info\")\n",
        "      btn.click(data_info_RESNET, outputs=[training_data_res, classes_res, test_data_res, test_accuracy_res])\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              model_name_res = gr.components.Textbox(label = \"Model Name\")\n",
        "              model_size_res = gr.components.Textbox(label = \"Model Size\")\n",
        "              image_source_res = gr.components.Textbox(label = \"Image Source\")\n",
        "      btn = gr.Button(\"Get Model Info\")\n",
        "      btn.click(model_info_RESNET, outputs=[model_name_res, model_size_res, image_source_res])\n",
        "\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              out_res = gr.components.Textbox()\n",
        "      btn = gr.Button(\"ResNet152V2 retrain\")\n",
        "      btn.click(resnet_train, outputs=[out_res])\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              out_cm_res = gr.components.Image(type='filepath')\n",
        "      btn = gr.Button(\"Confusion Matrix\")\n",
        "      btn.click(CM_RESNET, outputs=[out_cm_res])\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              seed_res = gr.components.Image(type='filepath')\n",
        "          with gr.Column():\n",
        "              out_res = gr.components.Image(type='filepath')\n",
        "              label = gr.components.Label(num_top_classes = 2)\n",
        "      btn = gr.Button(\"GradCAM\")\n",
        "      btn.click(save_and_display_gradcam_resnset, inputs=[seed_res], outputs=[out_res, label])\n",
        "      gr.Examples(sample_images, inputs=[seed_res])\n",
        "\n",
        "\n",
        "# # For InceptionResnet\n",
        "  with gr.Tab(\"InceptionResNetV2\"):\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              training_data_incres = gr.components.Textbox(label = \"Training Data\")\n",
        "              classes_incres = gr.components.Textbox(label = \"Number of Classes\")\n",
        "              test_data_incres = gr.components.Textbox(label = \"Testing Data\")\n",
        "              test_accuracy_incres = gr.components.Textbox(label = \"Test Accuracy\")\n",
        "      btn = gr.Button(\"Get Data Info\")\n",
        "      btn.click(data_info_INCRES, outputs=[training_data_incres, classes_incres, test_data_incres, test_accuracy_incres])\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              model_name_incres = gr.components.Textbox(label = \"Model Name\")\n",
        "              model_size_incres = gr.components.Textbox(label = \"Model Size\")\n",
        "              image_source_incres = gr.components.Textbox(label = \"Image Source\")\n",
        "      btn = gr.Button(\"Get Model Info\")\n",
        "      btn.click(model_info_INCRES, outputs=[model_name_incres, model_size_incres, image_source_incres])\n",
        "\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              out_incres = gr.components.Textbox()\n",
        "      btn = gr.Button(\"InceptionResNetV2 retrain\")\n",
        "      btn.click(inceptionResnet_train, outputs=[out_incres])\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              out_cm_incres = gr.components.Image(type='filepath')\n",
        "      btn = gr.Button(\"Confusion Matrix\")\n",
        "      btn.click(CM_INCRES, outputs=[out_cm_incres])\n",
        "\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              seed_incres = gr.components.Image(type='filepath')\n",
        "          with gr.Column():\n",
        "              out_incres = gr.components.Image(type='filepath')\n",
        "              label = gr.components.Label(num_top_classes = 2)\n",
        "      btn = gr.Button(\"GradCAM\")\n",
        "      btn.click(save_and_display_gradcam_incres, inputs=[seed_incres], outputs=[out_incres, label])\n",
        "      gr.Examples(sample_images, inputs=[seed_incres])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "WTXPWPG4nf2R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "22009927-2d38-4d4c-d94e-e885063cf255"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://98237fe5fd1d922f6c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://98237fe5fd1d922f6c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://98237fe5fd1d922f6c.gradio.live\n"
          ]
        }
      ]
    }
  ]
}